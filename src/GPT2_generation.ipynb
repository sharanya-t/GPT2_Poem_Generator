{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0-ENCzIKLAB"
   },
   "source": [
    "# Important Note:\n",
    "1. While the batched version of all generation functions are runnable (and more efficient), since they're using pre-sequence padding, you may get worse result from using them. We recommend you to set `batch_size` as 1 if you would like to run the code without any further modification / optimization.\n",
    "2. The rhyming-constrained functions are also runnable, but they're much less efficient than simply generating limericks without constraints and filtering the limericks in post-processing, also the rhyming words are limited by the words provided by `pronouncing` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wm3SvXWmNAHf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.17.0 in /home/sthilaga/.local/lib/python3.9/site-packages (4.17.0)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sthilaga/.local/lib/python3.9/site-packages (from transformers==4.17.0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (2022.3.15)\n",
      "Requirement already satisfied: requests in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /home/sthilaga/.local/lib/python3.9/site-packages (from transformers==4.17.0) (0.1.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/sthilaga/.local/lib/python3.9/site-packages (from transformers==4.17.0) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (4.63.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sthilaga/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sthilaga/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.17.0) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (3.3)\n",
      "Requirement already satisfied: click in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hydra-core in /home/sthilaga/.local/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /home/sthilaga/.local/lib/python3.9/site-packages (from hydra-core) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/sthilaga/.local/lib/python3.9/site-packages (from hydra-core) (4.9.3)\n",
      "Requirement already satisfied: packaging in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from hydra-core) (21.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging->hydra-core) (3.0.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pronouncing in /home/sthilaga/.local/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: cmudict>=0.4.0 in /home/sthilaga/.local/lib/python3.9/site-packages (from pronouncing) (1.0.15)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=5.1 in /home/sthilaga/.local/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (6.8.0)\n",
      "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /home/sthilaga/.local/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from importlib-metadata<7.0,>=5.1->cmudict>=0.4.0->pronouncing) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Start by installing required libraries (mainly Transformers)\n",
    "!pip install transformers==4.17.0\n",
    "!pip install scikit-learn\n",
    "!pip install hydra-core\n",
    "!pip install pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaPU47duNC7d"
   },
   "outputs": [],
   "source": [
    "# Only needed when running in colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ayf4KLuLRXUr"
   },
   "outputs": [],
   "source": [
    "!git clone https://{your_own_token}@github.com/coderalo/11785-automatic-poetry-generation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D-X1WieqOU1j"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pronouncing\n",
    "import random\n",
    "import shutil\n",
    "import string as string_utils\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import tqdm.notebook as tqdm\n",
    "import yaml\n",
    "\n",
    "from hydra import compose\n",
    "from hydra import initialize_config_dir\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Model\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y3vW53M_QeL6"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sys.path.append(\"/content/11785-automatic-poetry-generation/\")\n",
    "\n",
    "# from src.dataset import merge_lines, reorder, reverse_line\n",
    "# from src.dataset import LimerickDataset\n",
    "# from src.utils import load_dataset, get_tokenizer\n",
    "\n",
    "from dataset import merge_lines, reorder, reverse_line\n",
    "from dataset import LimerickDataset\n",
    "from utils import load_dataset, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IMc0kmrC7VP4"
   },
   "outputs": [],
   "source": [
    "def get_input_ids(\n",
    "        prompt,\n",
    "        tokenizer,\n",
    "        use_bos,\n",
    "        reverse,\n",
    "        add_line_token\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        prompt: str\n",
    "        tokenizer: the tokenizer used to generate tokens\n",
    "        use_bos: bool, use <BOS> token as the beginning of the prompt or not\n",
    "        reverse: bool, revert the word order or not\n",
    "        add_line_token: bool, add the <LINE> token at the end of prompt or not\n",
    "    Return:\n",
    "        input_ids: torch.LongTensor\n",
    "    \"\"\"\n",
    "    prompt = prompt.strip()\n",
    "    if add_line_token:\n",
    "        if prompt != \"\" and prompt[-6:] != \"<LINE>\":\n",
    "            prompt += \" <LINE>\"\n",
    "    if use_bos and prompt[:5] != \"<BOS>\":\n",
    "        prompt = \"<BOS> \" + prompt\n",
    "\n",
    "    if reverse is True:\n",
    "        input_ids = reverse_line(\n",
    "            input_ids=tokenizer(prompt, return_tensors=\"np\").input_ids[0],\n",
    "            use_bos=use_bos,\n",
    "            tokenizer=tokenizer,\n",
    "            reverse_last_line=True)\n",
    "        input_ids = torch.tensor(input_ids).reshape(1, -1)\n",
    "    else:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "57MlYMI7_pps"
   },
   "outputs": [],
   "source": [
    "def batch_decode(\n",
    "        outputs,\n",
    "        tokenizer,\n",
    "        use_bos,\n",
    "        reverse,\n",
    "        reverse_last_line\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        outputs: List of torch.LongTensor\n",
    "        tokenizer: the tokenizer used to decode tokens to words\n",
    "        use_bos: bool, whether the <BOS> token is used or not\n",
    "        reverse: bool, whether the tokens are in reverse order or not\n",
    "    \"\"\"\n",
    "    if reverse is True:\n",
    "        reversed = []\n",
    "        for output in outputs:\n",
    "            output = torch.tensor(\n",
    "                reverse_line(\n",
    "                    input_ids=output.cpu().numpy(),\n",
    "                    use_bos=use_bos,\n",
    "                    tokenizer=tokenizer,\n",
    "                    reverse_last_line=reverse_last_line)\n",
    "                ).reshape(-1)\n",
    "            reversed.append(output)\n",
    "        outputs = torch.stack(reversed)\n",
    "    else:\n",
    "        outputs = torch.stack(outputs)\n",
    "\n",
    "    outputs = tokenizer.batch_decode(outputs.cpu(), skip_special_tokens=False)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "v7vwQmBELzEj"
   },
   "outputs": [],
   "source": [
    "def count_lines(prompt):\n",
    "    return len(prompt.strip().split(\"<LINE>\")) - 1\n",
    "\n",
    "\n",
    "def lengths_to_mask(lengths, dtype, device, position=\"pos\"):\n",
    "    max_len = lengths.max().item()\n",
    "    if position == \"pos\":\n",
    "        mask = torch.arange(\n",
    "            max_len,\n",
    "            dtype=lengths.dtype,\n",
    "            device=lengths.device)\n",
    "        mask = mask.expand(len(lengths), max_len)\n",
    "        mask = (mask < lengths.unsqueeze(1))\n",
    "    else:\n",
    "        mask = torch.arange(\n",
    "            max_len - 1, -1, -1,\n",
    "            dtype=lengths.dtype,\n",
    "            device=lengths.device)\n",
    "        mask = mask.expand(len(lengths), max_len)\n",
    "        mask = (mask < lengths.unsqueeze(1))\n",
    "\n",
    "    mask = mask.clone().detach()\n",
    "    mask = mask.to(dtype=dtype, device=device)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BQLHA8LH4CSU"
   },
   "outputs": [],
   "source": [
    "def generate_lines(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        prompts,\n",
    "        generate_params,\n",
    "        num_generation,\n",
    "        batch_size,\n",
    "        add_line_token\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate / finish one line of the limerick. The prompts should be in the \n",
    "    correct word order (you don't need to revert the words before passing into\n",
    "    the function)\n",
    "    \"\"\"\n",
    "    use_bos = config.data.use_bos\n",
    "    reverse = config.data.reverse\n",
    "    order = config.data.order\n",
    "\n",
    "    \"\"\"\n",
    "    Step 1:\n",
    "        concat the input ids into a large tensor; notice that the prompts\n",
    "        are in variable lengths, thus we need to pad **before** the prompt,\n",
    "        and generate the attention mask accordingly\n",
    "    \"\"\"\n",
    "    full_input_ids = []\n",
    "    num_lines = []\n",
    "    for prompt in prompts:\n",
    "        num_lines = count_lines(prompt)\n",
    "        input_ids = get_input_ids(\n",
    "            prompt=prompt,\n",
    "            tokenizer=tokenizer,\n",
    "            use_bos=use_bos,\n",
    "            reverse=reverse,\n",
    "            add_line_token=add_line_token)\n",
    "        input_ids = input_ids.repeat(num_generation, 1)\n",
    "        full_input_ids.append(input_ids)\n",
    "\n",
    "    # generate attention mask\n",
    "    lengths = []\n",
    "    for input_ids in full_input_ids:\n",
    "        lengths += [input_ids.shape[1]] * input_ids.shape[0]\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    full_attention_mask = lengths_to_mask(lengths, torch.long, \"cpu\", \"pre\")\n",
    "\n",
    "    # pad the input ids\n",
    "    max_seq_len = max([input_ids.shape[1] for input_ids in full_input_ids])\n",
    "    full_input_ids = [\n",
    "        torch.cat([\n",
    "            torch.full(\n",
    "                (input_ids.shape[0], max_seq_len - input_ids.shape[1]),\n",
    "                fill_value=tokenizer.eos_token_id, dtype=torch.long\n",
    "            ),\n",
    "            input_ids\n",
    "        ], dim=1)\n",
    "        for input_ids in full_input_ids]\n",
    "    full_input_ids = torch.cat(full_input_ids, dim=0)\n",
    "\n",
    "    num_batches = math.ceil(full_input_ids.shape[0] / batch_size)\n",
    "\n",
    "    # assume that a line cannot be longer than 30 tokens\n",
    "    tmp_params = copy.deepcopy(generate_params)\n",
    "    if \"max_length\" in tmp_params:\n",
    "        tmp_params.pop(\"max_length\")\n",
    "    tmp_params[\"max_new_tokens\"] = 30\n",
    "\n",
    "    # Step 2: pass the batch into model to get generation output\n",
    "    outputs = []\n",
    "    for i in range(num_batches):\n",
    "    # for i in tqdm.trange(num_batches, leave=False):\n",
    "        input_ids = full_input_ids[i * batch_size: (i + 1) * batch_size]\n",
    "        input_ids = input_ids.to(device=config.device)\n",
    "        attention_mask = \\\n",
    "            full_attention_mask[i * batch_size: (i + 1) * batch_size]\n",
    "        attention_mask = attention_mask.to(device=config.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids, **tmp_params,\n",
    "                attention_mask=attention_mask,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "            output = torch.unbind(output)\n",
    "            outputs.extend(output)\n",
    "    \n",
    "    # Step 3: convert the generation result back to strings\n",
    "    outputs = batch_decode(\n",
    "        outputs=outputs,\n",
    "        tokenizer=tokenizer,\n",
    "        use_bos=use_bos,\n",
    "        reverse=reverse,\n",
    "        reverse_last_line=False)\n",
    "\n",
    "    clean_outputs = []\n",
    "    for output in outputs:\n",
    "        new_num_lines = count_lines(output)\n",
    "        if new_num_lines < num_lines + 1:\n",
    "            continue\n",
    "        output = output.strip().split(\" <LINE> \")[:num_lines + 1]\n",
    "        output = \" <LINE> \".join(output) + \" <LINE>\"\n",
    "        # clean up the prepended tokens\n",
    "        output = output.replace(\"<|endoftext|>\", \"\").strip()\n",
    "        clean_outputs.append(output)\n",
    "  \n",
    "    return clean_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ibjeKLl1M_Cy"
   },
   "outputs": [],
   "source": [
    "def generate_new_lines(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        prompts,\n",
    "        generate_params,\n",
    "        num_generation,\n",
    "        batch_size\n",
    "):\n",
    "    return generate_lines(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        config=config,\n",
    "        prompts=prompts,\n",
    "        generate_params=generate_params,\n",
    "        num_generation=num_generation,\n",
    "        batch_size=batch_size,\n",
    "        add_line_token=True)\n",
    "    \n",
    "\n",
    "def finish_lines(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        prompts,\n",
    "        generate_params,\n",
    "        num_generation,\n",
    "        batch_size\n",
    "):\n",
    "    return generate_lines(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        config=config,\n",
    "        prompts=prompts,\n",
    "        generate_params=generate_params,\n",
    "        num_generation=num_generation,\n",
    "        batch_size=batch_size,\n",
    "        add_line_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l8tbn1c1BttI"
   },
   "outputs": [],
   "source": [
    "def generate_limericks(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        prompts,\n",
    "        generate_params,\n",
    "        num_generation=10,\n",
    "        batch_size=1,\n",
    "        add_line_token=True,\n",
    "):\n",
    "    use_bos = config.data.use_bos\n",
    "    reverse = config.data.reverse\n",
    "    order = config.data.order\n",
    "\n",
    "    \"\"\"\n",
    "    Step 1:\n",
    "        concat the input ids into a large tensor; notice that the prompts\n",
    "        are in variable lengths, thus we need to pad **before** the prompts,\n",
    "        and generate the attention mask accordingly\n",
    "    \"\"\"\n",
    "    full_input_ids = []\n",
    "    num_lines = []\n",
    "    for prompt in prompts:\n",
    "        num_lines = count_lines(prompt)\n",
    "        input_ids = get_input_ids(\n",
    "            prompt=prompt,\n",
    "            tokenizer=tokenizer,\n",
    "            use_bos=use_bos,\n",
    "            reverse=reverse,\n",
    "            add_line_token=add_line_token)\n",
    "        input_ids = input_ids.repeat(num_generation, 1)\n",
    "        full_input_ids.append(input_ids)\n",
    "\n",
    "    # generate attention mask\n",
    "    lengths = []\n",
    "    for input_ids in full_input_ids:\n",
    "        lengths += [input_ids.shape[1]] * input_ids.shape[0]\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    full_attention_mask = lengths_to_mask(lengths, torch.long, \"cpu\", \"pre\")\n",
    "\n",
    "    # pad the input ids\n",
    "    max_seq_len = max([input_ids.shape[1] for input_ids in full_input_ids])\n",
    "    full_input_ids = [\n",
    "        torch.cat([\n",
    "            torch.full(\n",
    "                (input_ids.shape[0], max_seq_len - input_ids.shape[1]),\n",
    "                fill_value=tokenizer.eos_token_id, dtype=torch.long\n",
    "            ),\n",
    "            input_ids\n",
    "        ], dim=1)\n",
    "        for input_ids in full_input_ids]\n",
    "    full_input_ids = torch.cat(full_input_ids, dim=0)\n",
    "\n",
    "    num_batches = math.ceil(full_input_ids.shape[0] / batch_size)\n",
    "\n",
    "    # Step 2: pass the batch into model to get generation output\n",
    "    outputs = []\n",
    "    for i in range(num_batches):\n",
    "    # for i in tqdm.trange(num_batches, leave=False):\n",
    "        input_ids = full_input_ids[i * batch_size: (i + 1) * batch_size]\n",
    "        input_ids = input_ids.to(device=config.device)\n",
    "        attention_mask = \\\n",
    "            full_attention_mask[i * batch_size: (i + 1) * batch_size]\n",
    "        attention_mask = attention_mask.to(device=config.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids, **generate_params,\n",
    "                attention_mask=attention_mask,\n",
    "                pad_token_id=tokenizer.eos_token_id)\n",
    "            output = torch.unbind(output)\n",
    "            outputs.extend(output)\n",
    "\n",
    "    # Step 3: convert the generation result back to strings\n",
    "    outputs = batch_decode(\n",
    "        outputs=outputs,\n",
    "        tokenizer=tokenizer,\n",
    "        use_bos=use_bos,\n",
    "        reverse=reverse,\n",
    "        reverse_last_line=False)\n",
    "    clean_outputs = []\n",
    "\n",
    "    for output in outputs:\n",
    "        new_num_lines = count_lines(output)\n",
    "        if new_num_lines < 5:\n",
    "            continue\n",
    "        output = output.strip().split(\" <LINE> \")[:5]\n",
    "        output = \" <LINE> \".join(output) + \" <LINE>\"\n",
    "        # clean up the prepended tokens\n",
    "        output = output.replace(\"<|endoftext|>\", \"\").strip()\n",
    "        clean_outputs.append(output)\n",
    "\n",
    "    return clean_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hokwUlt_B9J8"
   },
   "outputs": [],
   "source": [
    "def generate_limericks_two_stage(\n",
    "        standard_lm,\n",
    "        reverse_lm,\n",
    "        standard_tokenizer,\n",
    "        reverse_tokenizer,\n",
    "        standard_config,\n",
    "        reverse_config,\n",
    "        prompts,\n",
    "        generate_params,\n",
    "        num_generation_1=10,\n",
    "        num_generation_2=1,\n",
    "        batch_size=64,\n",
    "):\n",
    "\n",
    "    first_lines = finish_lines(\n",
    "        model=standard_lm,\n",
    "        tokenizer=standard_tokenizer,\n",
    "        config=standard_config,\n",
    "        prompts=prompts,\n",
    "        generate_params=generate_params,\n",
    "        num_generation=num_generation_1,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    limericks = generate_limericks(\n",
    "        model=reverse_lm,\n",
    "        tokenizer=reverse_tokenizer,\n",
    "        config=reverse_config,\n",
    "        prompts=first_lines,\n",
    "        generate_params=generate_params,\n",
    "        num_generation=num_generation_2,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    return limericks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GQx0ZYxzEbgW"
   },
   "outputs": [],
   "source": [
    "def get_last_words(prompt):\n",
    "    prompt = prompt.split(' ')\n",
    "    \n",
    "    words = []\n",
    "    for i, word in enumerate(prompt):\n",
    "        if word == \"<LINE>\":\n",
    "            words.append(prompt[i - 1])\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_current_rhymes(prompt, tokenizer, allow_repetition=False):\n",
    "    num_lines = count_lines(prompt)\n",
    "    words = get_last_words(prompt)\n",
    "\n",
    "    try:\n",
    "        if num_lines in [0, 2]:  # first A or first B\n",
    "            return [], []\n",
    "        elif num_lines in [1, 4]:  # 2nd and 3rd A in AABBA\n",
    "            if num_lines == 1:\n",
    "                words = [words[0]]\n",
    "            else:\n",
    "                words = [words[0], words[1]]\n",
    "        elif num_lines == 3:\n",
    "            words = [words[2]]\n",
    "    except Exception:\n",
    "        words = []\n",
    "        rhyme_tokens, rhymes = [], []\n",
    "        return rhyme_tokens, rhymes\n",
    "\n",
    "    rhymes = set()\n",
    "    for word in words:\n",
    "        rhymes.update(pronouncing.rhymes(word))\n",
    "    if not allow_repetition:\n",
    "        for word in words:\n",
    "            if word in rhymes:\n",
    "                rhymes.remove(word)\n",
    "    rhymes = list(rhymes)\n",
    "\n",
    "    if rhymes != []:\n",
    "        rhyme_tokens = [\n",
    "            rhyme[::-1] for rhyme in tokenizer(rhymes)['input_ids']]\n",
    "    else:\n",
    "        rhyme_tokens = []\n",
    "\n",
    "    return rhyme_tokens, rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d_-v86_IJmnx"
   },
   "outputs": [],
   "source": [
    "def pad_tokens(tokens, tokenizer, max_len):\n",
    "    padded_tokens = [\n",
    "        tokens_ + [tokenizer.pad_token_id] * (max_len - len(tokens_))\n",
    "        for tokens_ in tokens]\n",
    "    attention_mask = [\n",
    "        [1.] * len(tokens_) + [0.] * (max_len - len(tokens_))\n",
    "        for tokens_ in tokens]\n",
    "\n",
    "    padded_tokens = torch.tensor(padded_tokens, dtype=torch.long)\n",
    "    attention_mask = torch.tensor(attention_mask, dtype=torch.float)\n",
    "\n",
    "    return padded_tokens, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H69LJx2NJqBb"
   },
   "outputs": [],
   "source": [
    "def get_rhyming_word_score(\n",
    "        reverse_lm,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        prompts,\n",
    "        rhymes,\n",
    "        temperature,\n",
    "        batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 1: \n",
    "        generate input ids for each prompts (not concatenated now)\n",
    "        also collect the max rhyme (tokens) len for next step\n",
    "    \"\"\"\n",
    "    lengths, max_rhyme_len = [], 0\n",
    "    input_ids_list = []\n",
    "    for prompt, rhymes_ in zip(prompts, rhymes):\n",
    "        input_ids = get_input_ids(\n",
    "            prompt=prompt,\n",
    "            tokenizer=tokenizer,\n",
    "            use_bos=config.data.use_bos,\n",
    "            reverse=True,\n",
    "            add_line_token=True)\n",
    "        \n",
    "        # [l_0, ..., l_0, l_1, ..., l_1, ...]\n",
    "        lengths.extend([input_ids.shape[1]] * len(rhymes_))\n",
    "        input_ids = input_ids.repeat(len(rhymes_), 1)\n",
    "        input_ids_list.append(input_ids)\n",
    " \n",
    "        rhyme_len = max([len(rhyme) for rhyme in rhymes_])\n",
    "        max_rhyme_len = max(max_rhyme_len, rhyme_len)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 2:\n",
    "        generate input ids for each rhyme word list to concat with prompts\n",
    "        the attention mask is generated to calculate the scores later\n",
    "    \"\"\"\n",
    "    padded_rhymes_list = []\n",
    "    rhyme_masks = []\n",
    "    for rhymes_ in rhymes:\n",
    "        padded_rhymes, attention_mask = \\\n",
    "            pad_tokens(rhymes_, tokenizer, max_rhyme_len)\n",
    "        padded_rhymes_list.append(padded_rhymes)\n",
    "        rhyme_masks.append(attention_mask)\n",
    "\n",
    "    padded_rhymes = torch.cat(padded_rhymes_list, dim=0)\n",
    "    rhyme_masks = torch.cat(rhyme_masks, dim=0)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 3:\n",
    "        concat the input ids of prompts with rhyme words\n",
    "        also need to pad them to the same length for batching\n",
    "    \"\"\"\n",
    "    input_ids_list = [\n",
    "        torch.cat([input_ids, padded_rhymes], dim=1)\n",
    "        for input_ids, padded_rhymes in\n",
    "        zip(input_ids_list, padded_rhymes_list)]\n",
    "\n",
    "    max_seq_len = max([input_ids.shape[1] for input_ids in input_ids_list])\n",
    "    input_ids_list = [\n",
    "        torch.cat(\n",
    "            [\n",
    "                input_ids,\n",
    "                torch.full(\n",
    "                    (input_ids.shape[0], max_seq_len - input_ids.shape[1]),\n",
    "                    fill_value=tokenizer.pad_token_id,\n",
    "                    dtype=torch.long, device=\"cpu\")\n",
    "            ], dim=1)\n",
    "        for input_ids in input_ids_list]\n",
    "\n",
    "    full_input_ids = torch.cat(input_ids_list, dim=0)\n",
    "    num_examples = full_input_ids.shape[0]\n",
    "    num_batches = math.ceil(num_examples / batch_size)\n",
    "\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    total_lengths = lengths + max_rhyme_len\n",
    "    attention_masks = lengths_to_mask(total_lengths, torch.float, \"cpu\")\n",
    "\n",
    "    \"\"\"\n",
    "    Step 4:\n",
    "        pass the batches into model to get logits, which then are converted\n",
    "        into log probs and aggregated to get the final scores\n",
    "    \"\"\"\n",
    "    full_scores = []\n",
    "    for i in tqdm.trange(num_batches, leave=False):\n",
    "        input_ids = full_input_ids[i * batch_size: (i + 1) * batch_size]\n",
    "        attention_mask = attention_masks[i * batch_size: (i + 1) * batch_size]\n",
    "        input_ids = input_ids.to(device=config.device)\n",
    "        attention_mask = attention_mask.to(device=config.device)\n",
    "  \n",
    "        batch_lengths = lengths[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_padded_rhymes = \\\n",
    "            padded_rhymes[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_rhyme_masks = rhyme_masks[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        batch_padded_rhymes = batch_padded_rhymes.to(device=config.device)\n",
    "        batch_rhyme_masks = batch_rhyme_masks.to(device=config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = reverse_lm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask)['logits']\n",
    "\n",
    "            # [batch_size, max_rhyme_len]\n",
    "            offsets = (torch.arange(0, input_ids.shape[0]) * max_seq_len)\n",
    "            offsets = offsets.reshape(-1, 1).repeat(1, max_rhyme_len)\n",
    "            indices = (offsets + batch_lengths.reshape(-1, 1)).reshape(-1)\n",
    "            indices = indices.to(device=config.device)\n",
    "\n",
    "            # [batch_size * max_seq_len, vocab_size]\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "            # [batch_size * max_rhyme_len, vocab_size]\n",
    "            logits = torch.index_select(logits, 0, indices)\n",
    "            # [batch_size, max_rhyme_len, vocab_size]\n",
    "            logits = logits.reshape(input_ids.shape[0], max_rhyme_len, -1)\n",
    "\n",
    "            log_probs = F.softmax(logits, -1)\n",
    "            # [batch_size, max_rhyme_len]\n",
    "            scores = torch.gather(\n",
    "                log_probs, 2,\n",
    "                batch_padded_rhymes.unsqueeze(2)).squeeze()\n",
    "            scores = torch.sum(scores * batch_rhyme_masks, dim=1)\n",
    "            scores = scores.cpu().numpy()\n",
    "\n",
    "            full_scores.append(scores)\n",
    "\n",
    "    scores = np.concatenate(full_scores, axis=0)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 5:\n",
    "        split the final results back into array for each prompt\n",
    "    \"\"\"\n",
    "    probs_list, anchor = [], 0\n",
    "    for rhymes_ in rhymes:\n",
    "        probs = scores[anchor: anchor + len(rhymes_)]\n",
    "        probs /= np.sum(probs)\n",
    "        probs_list.append(probs)\n",
    "        anchor += len(rhymes_)\n",
    "\n",
    "    return probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MWb_21RiEcHO"
   },
   "outputs": [],
   "source": [
    "def attach_next_rhyming_word(\n",
    "        reverse_lm,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        prompts,\n",
    "        num_samples,\n",
    "        weighted,\n",
    "        temperature=None,\n",
    "        batch_size=64\n",
    "):\n",
    "    prompts_with_next_word = [None for _ in prompts]\n",
    "    prompts_with_rhymes, prompts_without_rhymes = [], []\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        tokens, words = get_current_rhymes(prompt, tokenizer)\n",
    "        if tokens != []:\n",
    "            prompts_with_rhymes.append([idx, prompt, tokens, words])\n",
    "        else:\n",
    "            prompts_without_rhymes.append([idx, prompt])\n",
    "\n",
    "    if weighted and prompts_with_rhymes != []:\n",
    "        probs_list = get_rhyming_word_score(\n",
    "            reverse_lm=reverse_lm,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config,\n",
    "            prompts=[p[1] for p in prompts_with_rhymes],\n",
    "            rhymes=[p[2] for p in prompts_with_rhymes],\n",
    "            temperature=(1.0 if temperature is None else temperature),\n",
    "            batch_size=batch_size)\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        probs_list = [\n",
    "            np.ones(len(p[3])) / len(p[3])\n",
    "            for p in prompts_with_rhymes]\n",
    "\n",
    "    for prompt_info, probs in zip(prompts_with_rhymes, probs_list):\n",
    "        idx, prompt, _, words = prompt_info\n",
    "        samples = np.random.choice(len(words), num_samples, p=probs)\n",
    "        prompts_with_next_word[prompt_info[0]] = \\\n",
    "            [f\"{prompt} {words[s]}\" for s in samples]\n",
    "\n",
    "    for idx, prompt in prompts_without_rhymes:\n",
    "        prompts_with_next_word[idx] = [prompt] * num_samples\n",
    "\n",
    "    prompts_with_next_word = [\n",
    "        prompt for prompts in prompts_with_next_word\n",
    "        for prompt in prompts]\n",
    "\n",
    "    return prompts_with_next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iY03gaA0EfDs"
   },
   "outputs": [],
   "source": [
    "def generate_limericks_with_rhyming(\n",
    "        reverse_lm,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        prompts,\n",
    "        generate_params,\n",
    "        weighted,\n",
    "        num_generation=10,\n",
    "        batch_size=10\n",
    "):\n",
    "    \n",
    "    limericks = []\n",
    "    prompt = \"\"\n",
    "\n",
    "    prompts = generate_new_lines(\n",
    "        model=reverse_lm,\n",
    "        tokenizer=tokenizer,\n",
    "        config=config,\n",
    "        prompts=prompts,\n",
    "        generate_params=generate_params,\n",
    "        num_generation=num_generation,\n",
    "        batch_size=batch_size)\n",
    "  \n",
    "    for prompt in prompts:\n",
    "        print(prompt)\n",
    "    \n",
    "    for _ in range(4):\n",
    "        new_prompts = attach_next_rhyming_word(\n",
    "            reverse_lm=reverse_lm,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config,\n",
    "            prompts=prompts,\n",
    "            num_samples=1,\n",
    "            weighted=weighted,\n",
    "            temperature=1.0)\n",
    "        prompts = finish_lines(\n",
    "            model=reverse_lm,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config,\n",
    "            prompts=new_prompts,\n",
    "            generate_params=generate_params,\n",
    "            num_generation=1,\n",
    "            batch_size=batch_size)\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            print(prompt)\n",
    "        \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NjBASX6sJxLl"
   },
   "outputs": [],
   "source": [
    "def generate_limericks_two_stage_with_rhyming(\n",
    "        standard_lm,\n",
    "        reverse_lm,\n",
    "        standard_tokenizer,\n",
    "        reverse_tokenizer,\n",
    "        standard_config,\n",
    "        reverse_config,\n",
    "        prompts,\n",
    "        generate_params,\n",
    "        weighted,\n",
    "        num_generation_1=10,\n",
    "        num_generation_2=1,\n",
    "        batch_size=1,\n",
    "):\n",
    "    lines = finish_lines(\n",
    "        model=standard_lm,\n",
    "        tokenizer=standard_tokenizer,\n",
    "        config=standard_config,\n",
    "        prompts=prompts,\n",
    "        generate_params=generate_params,\n",
    "        num_generation=num_generation_1,\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    for _ in range(4):\n",
    "        lines = attach_next_rhyming_word(\n",
    "            reverse_lm=reverse_lm,\n",
    "            tokenizer=reverse_tokenizer,\n",
    "            config=reverse_config,\n",
    "            prompts=lines,\n",
    "            num_samples=1,\n",
    "            weighted=weighted,\n",
    "            temperature=1.0)\n",
    "        lines = finish_lines(\n",
    "            model=reverse_lm,\n",
    "            tokenizer=reverse_tokenizer,\n",
    "            config=reverse_config,\n",
    "            prompts=lines,\n",
    "            generate_params=generate_params,\n",
    "            num_generation=1,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ugo3W1OZB1pE"
   },
   "outputs": [],
   "source": [
    "def load_model(exp_dir, tmp_root=\"/scratch/sthilaga/GPT2_Poem_Generator/content/test/\"):\n",
    "    config = OmegaConf.create(yaml.safe_load(open(exp_dir + \"/config.yaml\")))\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(f\"{exp_dir}/tokenizer\")\n",
    "\n",
    "    if not os.path.exists(tmp_root):\n",
    "        print(\"Path does not exist! :(\")\n",
    "        # os.makedirs(tmp_root, exist_ok=True)\n",
    "    tmp_dir = tempfile.mkdtemp(dir=tmp_root)\n",
    "    states = torch.load(f\"{exp_dir}/best-model.ckpt\")\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.cuda()\n",
    "    model.load_state_dict(states['model_state_dict'])\n",
    "    model.save_pretrained(tmp_dir)\n",
    "    new_model = AutoModelForCausalLM.from_pretrained(tmp_dir)\n",
    "    new_model = new_model.cuda()\n",
    "\n",
    "    return config, tokenizer, new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jy3rVKpnSMp7"
   },
   "source": [
    "## Example of one-stage generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kOsFVEDaB5HC"
   },
   "outputs": [],
   "source": [
    "exp_dir = f\"/scratch/sthilaga/GPT2_Poem_Generator/config/reverse-gpt2\"\n",
    "config, tokenizer, model = load_model(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1EmhJ1QDB61V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> who's living in paris or moma <LINE> she's determined to live on a chammy <LINE> and so does his life <LINE> the needs of his wife <LINE> she says he sleeps out through the mammy <LINE>\n",
      "<BOS> in the '60s are all et vie-cee <LINE> and we marched in, with much too much glee <LINE> early time that we fought <LINE> where they once looted and burned <LINE> to the tealas we fled; we were free <LINE>\n",
      "<BOS> it's plural (a word that is called a) <LINE> aorto-ment? a kind of aawa <LINE> by the french u.k. <LINE> all the experts agree <LINE> not a home vacation in nova <LINE>\n",
      "<BOS> here's an ogerman, auger? right for it <LINE> he's some better, no longer fight for it <LINE> just be sure, for a guy <LINE> an officer's eye <LINE> for that. that's why he will fight for it <LINE>\n",
      "<BOS> i was probed, on the head of this leafy <LINE> that on earth. and this new christian theory <LINE> and a true young ahexis <LINE> have purged on my poshysys <LINE> don't need not be duped. i be wary <LINE>\n",
      "<BOS> young helen, was first and first found <LINE> to a girl that was carnally bound <LINE> the knot tightly bound <LINE> with the young man tied 'round <LINE> all his baggage was carried around <LINE>\n",
      "<BOS> pinal lusts for the captain (toyor) <LINE> with her bowsen, set the sail now for <LINE> and from bows to the captain <LINE> men are holding him in <LINE> they'll follow the course that's his vow for <LINE>\n",
      "<BOS> the p. eden's a shellfish, it's said <LINE> it's edens to which one shell is fed <LINE> that the male as a mate <LINE> for each mate, for her mate <LINE> first it dies and might make the dead bed <LINE>\n",
      "<BOS> when we're criss-crossing the beauties today <LINE> we bring presents a good day by sunday <LINE> and all of our grasses <LINE> are two grassy masses <LINE> for then, once it's wednesday, for sunday <LINE>\n",
      "<BOS> as is an ambiguous word <LINE> is as easy as i've ever heard <LINE> but to swear it is true <LINE> a synonym, like you <LINE> and it's highly often preferred <LINE>\n",
      "<BOS> (you're ammoniferous, we know <LINE> but you're tough, and you're likely to grow <LINE> with the size of your shell <LINE> it's just acid as well <LINE> but it isn't a glow and a glow <LINE>\n",
      "<BOS> this odd word definable's found <LINE> below, or above, or around <LINE> here's its meaning that is <LINE> it's pithy, and so it's <LINE> as a clear definible sound <LINE>\n",
      "<BOS> half asleep, and he's downing all day <LINE> so we ask him to be gone away <LINE> should we break into bed <LINE> and bring back to his head <LINE> then again, he broke off in that way <LINE>\n",
      "<BOS> use a medic; describe it with meds <LINE> but to learn how to do what he needs <LINE> it may be the treatment, doc <LINE> that to help to detox <LINE> just one of those growing in weeds <LINE>\n",
      "<BOS> i'm a diva? well, that's what i've seen <LINE> a young man, though, i'm gray and i'm green <LINE> if you're up and you see <LINE> you'd be looking for me <LINE> if i'm here, you'd regard the obscene <LINE>\n",
      "<BOS> though london's operas were great <LINE> brought his bravo: the critics would hate <LINE> and the opera he wrote <LINE> yet he gave us a note <LINE> it was only a flowery state <LINE>\n",
      "<BOS> when i'm sick of this hooker i know <LINE> use the cetson, which doesn't show <LINE> i've the bite of the cat <LINE> and he'll often fall flat <LINE> for i keep chletting out all my woe <LINE>\n",
      "<BOS> two's, three's, fours, and fours <LINE> and in war, and in war, and in war <LINE> and in war number two <LINE> also ceo (u!) <LINE> this list is what we all ignore <LINE>\n",
      "<BOS> kids are hard to have sex? ain't so great <LINE> i can watch them while i feel straight <LINE> though i know it's boys, too <LINE> (but when asking for you) <LINE> my condom. so now i must wait <LINE>\n",
      "<BOS> please find him, i don't need to know it <LINE> (rip me off; there's no reason to show it <LINE> if it lets me berate him <LINE> i can say i just hate him <LINE> please, don't berate him, accuse me, i prude it <LINE>\n",
      "<BOS> the affluent, wealthy young jews <LINE> where the gifts and the money they choose <LINE> these things might be nice <LINE> but take my advice <LINE> before shopping at home in caboose <LINE>\n",
      "<BOS> there once was a lady named joe <LINE> and a job that her job said to go <LINE> had her tedious work <LINE> her nerves went berserk <LINE> but just blew her off and know why? no <LINE>\n",
      "<BOS> the atmosphere's sun's emanation <LINE> a diastron appears at its station <LINE> in a bright outer space <LINE> once we've moved to matter's place <LINE> from the past and, forever, stagnation <LINE>\n",
      "<BOS> my sex work is hard, i must say <LINE> when alone, there's a semblance of sway <LINE> but i'm out of my mind <LINE> it makes me feel unkind <LINE> that's what peace isn't likely to fray <LINE>\n",
      "<BOS> you're good-natured and artless, no doubt <LINE> you've no talent for keeping me out <LINE> don't you think that my work <LINE> and you junkie, the jerk <LINE> that's a fact, oh my god, nobody's spout <LINE>\n",
      "<BOS> can you bury it? god, please don't doubt it <LINE> can you use it. i don't care to cast out it <LINE> i did nothing to say <LINE> won't you see it. i'm ok <LINE> so i cry? you might call it a basket <LINE>\n",
      "<BOS> these bacteria (biferous) <LINE> are good, not for me aphidiferous <LINE> some foods that i need <LINE> as for all i would feed <LINE> to eat food that's quite corniferous <LINE>\n",
      "<BOS> but you're late, we went out today <LINE> i have served us some fishy! no way <LINE> for the eggs and the fish <LINE> keeps us using this dish <LINE> have you found here, we feel like a buffet <LINE>\n",
      "<BOS> they're a family of plants, i would see <LINE> pink-like leaves that emerge from this tree <LINE> pink-like flowers, bright <LINE> like a red, heaven-fright <LINE> and their own blooms are names of just three <LINE>\n",
      "<BOS> if you follow the needle, you'll find <LINE> now that force you to go and should bind <LINE> all the thread end to end <LINE> through your small needle's bend <LINE> as the lines as the points are combined <LINE>\n",
      "<BOS> my husband, when he was wedded <LINE> has awoke and fell out with his bed <LINE> very late in the day <LINE> he would happily play <LINE> with a couple of husbands instead <LINE>\n",
      "<BOS> i loved games that he put on a play <LINE> he loved games and played games every day <LINE> with his mom and his wife <LINE> what great fame was his life <LINE> and rolled up in a different way <LINE>\n",
      "<BOS> we're under the depths of the sea <LINE> this rich, sticky soil, like a tree <LINE> and bright leaves, of flowers <LINE> is lovely, for hours <LINE> with flowery blooms like two or three <LINE>\n",
      "<BOS> tell yer? i don't want to tell yer <LINE> totelltell yer? here's the truth, i totell yer <LINE> this fact, i can't tell <LINE> i'm real tryin' in hell <LINE> i would guess that you'd be a totell yer <LINE>\n",
      "<BOS> our circus has brought us a fling <LINE> up on stage. with them, no one can sing <LINE> each pose of the clown <LINE> and a large and small clown <LINE> who's the looker? the funniest thing <LINE>\n",
      "<BOS> anthemics, most folks are confessing <LINE> and they seldom indulge in good dressing <LINE> associative cools <LINE> nonalcohol, drugs <LINE> than some more of the drugs that you're guessing <LINE>\n",
      "<BOS> this expressway's east to australia <LINE> don't complain that it's north to australia <LINE> if you run the wrong way <LINE> why not go in the sown way <LINE> in caaustralia, you won't get australia <LINE>\n",
      "<BOS> we're skinnish. some say <LINE> young june's, by far, far far away <LINE> she is old, though, we dweller <LINE> and endearably sweller <LINE> but the winter is, now, one fine day <LINE>\n",
      "<BOS> the holes in my pasty?that's got 'er <LINE> and their crusts?now, well, where can i spot 'er <LINE> as i blow through the hole <LINE> things crawl out through a knoll <LINE> and my truck (like a dodderwheel tractor) <LINE>\n",
      "<BOS> i am sitting here, every day <LINE> i am working quite well, so they say <LINE> i cannot awake <LINE> yet it's best just to make <LINE> go and take a few minutes? okay <LINE>\n",
      "<BOS> an india (v) is his name <LINE> in augusta (he gained lasting fame <LINE> was a genius at first <LINE> great greatness. it's cursed <LINE> in high times. the nobility's game <LINE>\n",
      "<BOS> when at breakfast before i would play <LINE> i'd feel sad that the day's not ok <LINE> now, just as a snack <LINE> who will not seem to crack <LINE> well, i will just stand up to say <LINE>\n",
      "<BOS> according to me (coneuphic) <LINE> write a book just for you?my (how cryptic) <LINE> yes, five lines of blank verse <LINE> each limerick is terse <LINE> i can find it for life; i'm a kithic <LINE>\n",
      "<BOS> i love crannies? nothing but faze yer <LINE> that they're well, and not up to praise yer <LINE> but i'll never deny it <LINE> buy it! now, let's try it <LINE> now for me, there's also a fav'rerry <LINE>\n",
      "<BOS> a camboclet's part of a tree <LINE> it's a fruit that's lovely for me <LINE> as a word (by its name) <LINE> filled with blossoms of fame <LINE> think it blossom?that's fruit's on the sea <LINE>\n",
      "<BOS>? that's acceptable <LINE> and that is a fact, you're acceptable <LINE> but accepted is all <LINE> it will show you such gall <LINE> does the truth, tells the truth, you're mendable <LINE>\n",
      "<BOS> a sentence? you ought to precede <LINE> two words with respect and rapport <LINE> that's a strong word to use <LINE> your attention and choose <LINE> that last word to which you speak of rapport <LINE>\n",
      "<BOS> a voice that's, cattymically, said <LINE> using a means to come to the dead <LINE> someone's accent is loud <LINE> for the deafening crowd <LINE> haven't heard someone's much louder instead <LINE>\n",
      "<BOS> though that jane is a basket for me <LINE> like a pretty young guy, and i was he <LINE> so i threw the kid in <LINE> the thought made me grin <LINE> perhaps that is just why he's curt <LINE>\n",
      "<BOS> it's demonstrable: yes, it's true <LINE> a fact is a fact i eschew <LINE> though that fact is demonstrable <LINE> it's so demonstrable <LINE> are you up in old english peru <LINE>\n",
      "<BOS> she looks gorgeous; she's pretty and sweet <LINE> she can wobble and leap off her feet <LINE> when i'm picking her up <LINE> i'll fill her for a cup <LINE> with some fingerfood's munchie to eat <LINE>\n",
      "<BOS> my darling; it's funny to me <LINE> and it drives me away from the sea <LINE> like a dog that is dishy <LINE> it's tasty, and fishy <LINE> as cute as fish food?twenty-three <LINE>\n",
      "<BOS> they say <LINE> they're all of my best friends. by the way <LINE> is my darling, my dear <LINE> for to stay, far and near <LINE> and it makes me feel airy, we're gay <LINE>\n",
      "<BOS> the label appears that you're nappin' <LINE> under sugar, and taste all a-tappin' <LINE> but beware. now you say it <LINE> it's not in your diet <LINE> you're sorry it won't let this happen <LINE>\n",
      "<BOS> i'm a gift for this picky old hottie <LINE> she's as skinny and tall as an tootty <LINE> she is manly and cute <LINE> and she looks like a soot <LINE> and wears thickly instead of a potty <LINE>\n",
      "<BOS> al is wood that is found in a tree <LINE> that grows very thick?i agree <LINE> it has leaves, nut and bark <LINE> when i sleep in the dark <LINE> my bark tree's as good as can be <LINE>\n",
      "<BOS> what we do, though not, have no fear <LINE> it's against us: you're toothless, my dear <LINE> kick it out. work it out <LINE> does your lips are about <LINE> that, after all, it sprouts appear <LINE>\n",
      "<BOS> this fungus, which now will be ticked <LINE> it's a place where many plants have been junked <LINE> for this fungal disease <LINE> from your joints to your knees <LINE> have it killed you, alone and extinct <LINE>\n",
      "<BOS> city agents arriving today <LINE> town agents are running away <LINE> agents coming out there <LINE> when they stink just like air <LINE> and just like it makes something okay <LINE>\n",
      "<BOS> i'm in love with you, i fear <LINE> wake up? please call on me, my dear <LINE> but if you marry me <LINE> though you would not bury me <LINE> no wonder i'm looking out here <LINE>\n",
      "<BOS> oh, man! i should appraise you <LINE> that you object in love. what, i do <LINE> it's for me as life hell <LINE> to describe me as well <LINE> a young woman then see you anew <LINE>\n",
      "<BOS> in the fall in our state, one would say <LINE> some saved millions, this earth came the way <LINE> up the desert and fell <LINE> this land's fate, i could tell <LINE> and the fell?oh, that's all... and... hey <LINE>\n",
      "<BOS> my synonym's use may be scanty <LINE> a synonym, by which i'm vesty <LINE> it is only the name <LINE> to explain just the same <LINE> synonym is mine to be kenty <LINE>\n",
      "<BOS> there once was a girl, and i'd say <LINE> who wrote stories about all the way <LINE> she was one of those tales <LINE> to a couple of gales <LINE> but in death. (how now life is at bay <LINE>\n",
      "<BOS> defunctuation occurs in a word <LINE> when it's directly proved or inferred <LINE> either suffix is used <LINE> who sure isn't amused <LINE> if a suffix's the word to be heard <LINE>\n",
      "<BOS> you've seen us, and you've seen us, and you've seen us <LINE> you're obscene? it's obscene us <LINE> my devotion to you <LINE> my devotion to you <LINE> i wed you; i call you for venus <LINE>\n",
      "<BOS> she's a lady, is hard not to say <LINE> far too far, one is near, far away <LINE> i would never adore her <LINE> she'd wed my young daughter <LINE> let me give her up, give her the day <LINE>\n",
      "<BOS> when, in prison, the person is dissed <LINE> does not think he is hard to resist <LINE> he's not locked out of jail <LINE> he won't find him?without bail <LINE> he will find him in jail no one's missed <LINE>\n",
      "<BOS> i could make lots of chocolate for me <LINE> and would take a big cup of tea <LINE> it'll be hot because i <LINE> if the flavor's awry <LINE> you need help? help me get chocolate free <LINE>\n",
      "<BOS> this bogeyman's stuck in his hand. it <LINE> note: a goshie, a gushie, a bandit <LINE> with one note we see <LINE> in the f. s.o.b <LINE> and b?as, of course, we understand it <LINE>\n",
      "<BOS> a phrase could say fads or your nouns <LINE> when you ask them, you'd get fads to nerds <LINE> or so if you might say <LINE> when they're not, either way <LINE> that's no way that would get fads to nerds <LINE>\n",
      "<BOS> astronism's simple, it's true <LINE> light goes down?it's as far as we're through <LINE> when that light, as i see <LINE> you may watch your tv <LINE> feel it way out of light, through and through <LINE>\n",
      "<BOS> a mineralite miller, my man <LINE> was carbon when combustion began <LINE> he would burn it, when venting <LINE> his lungs; now he's clotting <LINE> was fit to the chemist's plan <LINE>\n",
      "<BOS> you go for an act of elation <LINE> i attend to you deep contemplation <LINE> is it there, here or there <LINE> but i really don't care <LINE> for your purposeful lust and flirtation <LINE>\n",
      "<BOS> well, where is this? well (in the rhyme) <LINE> we know we're, well, bedazzling some chimes <LINE> in our class (yes, we dance) <LINE> but a light pair of pants <LINE> that will sometimes wind up with good times <LINE>\n",
      "<BOS> i'm a godfather god (yeah, i'd say) <LINE> as a godman, by god is my way <LINE> it's what god i should see <LINE> my love life out to me <LINE> it's a person that means to convey <LINE>\n",
      "<BOS> they're in classical music, they say <LINE> when most songs are performed in the day <LINE> when you have to be heard <LINE> they are simply absurd <LINE> (like if used in a bowl made of clay. <LINE>\n",
      "<BOS> do your baldness? we feel quite unculler <LINE> oh hey, what's it; that's just the better <LINE> of skin. have your hair <LINE> but with ways that are fair <LINE> you're a man! and if you are a brother <LINE>\n",
      "<BOS> the word autobatic's a word <LINE> for a vowel. how strange and absurd <LINE> but it isn't that clear <LINE> which is this and that's here <LINE> to a word number one, you've concurred <LINE>\n",
      "<BOS> when i'm doing too much, i could say <LINE> at a party, i've done it all day <LINE> it brings presents to me <LINE> in 103 <LINE> check my anapest?listing's my way <LINE>\n",
      "<BOS> in the '60s, i'm some kind of screamer <LINE> i'm a skimmer, these days, is gossamer <LINE> like cool air, crystal clear <LINE> for i swear i've no peer <LINE> but i love it? i'm just too much glossamer <LINE>\n",
      "<BOS> see me, kid, i had wandered around. it <LINE> as far as i ever happily found it <LINE> i'm belittling my feet <LINE> and get fit in a seat <LINE> and get pickled, i'd wandered around it <LINE>\n",
      "<BOS> won't you please, i should do it, you <LINE> my grades are ninety and sixty-two <LINE> but now twenty am i <LINE> and is high, i'm not high <LINE> i'm glad that today i turned blue <LINE>\n",
      "<BOS> i don't drink it? can't you <LINE> i don't drink it? i think it? can you <LINE> i don't drink i don't drink <LINE> yes, my brain's in the pink <LINE> i can't feel it or i stink it? can you <LINE>\n",
      "<BOS> go mix up latin, don't be great <LINE> it's best if you're a tempting fate <LINE> mix stew or just drink <LINE> i could kill you. i think <LINE> would you get it right onto your plate <LINE>\n",
      "<BOS> an apostrophe's a head for the head <LINE> it's a nautical part that's to read <LINE> of nouns, so they say it <LINE> a good printed paper <LINE> it, and can't take on, or the dead <LINE>\n",
      "<BOS> i expect you to eat it, i miss'll <LINE> an anti-guided missile missile <LINE> i must aim very well <LINE> shoot me up. please don't tell <LINE> the judge who will get the dismissal <LINE>\n",
      "<BOS> the king, who has heard what i said <LINE> born in ireland. the throne was his head <LINE> (ten years later), then came <LINE> it was caesar's great fame <LINE> a place where the world once had spread <LINE>\n",
      "<BOS> bryamics, i have to agree <LINE> is not decoetalism. it's me <LINE> seems my love (for it's true) <LINE> that i'm just like me and you <LINE> i emulcorate nothing to thee <LINE>\n",
      "<BOS> no?what?what!?no! i needn't deny it <LINE> is it? you just want to deny it <LINE> you getaway from me <LINE> to the highest degree <LINE> it? is it? you're hard not to fit <LINE>\n",
      "<BOS> aerobics, us? now we can't see <LINE> that you breathe in between you and me <LINE> now propel you on air <LINE> even then i don't care <LINE> i just fart in you on the tv <LINE>\n",
      "<BOS> you say <LINE> to embellish, you'll know every day <LINE> here is what you will do <LINE> that will fill one or two <LINE> in a straightforward, natural way <LINE>\n",
      "<BOS> backalskiness's good, all agree <LINE> and your work is as hard as can be <LINE> but wherever you go <LINE> you will spend all your dough <LINE> for just one of a firm will agree <LINE>\n",
      "<BOS> cursableness is common, they say <LINE> using cursiveness, men's on the way <LINE> and women, we wear <LINE> about men. do we dare <LINE> to the girls? 'cause we feel rather gay <LINE>\n",
      "<BOS> criss-crossing a theater, at play <LINE> while a host has a room and you pay <LINE> to wonder that you <LINE> the company eats you <LINE> for i'm better, he spreads it away <LINE>\n",
      "<BOS> my grandson, who's barf's and looks great <LINE> to a big age, he's six figure eight <LINE> he's five inches tall <LINE> with short arms, and won't laugh <LINE> looking older, man! well, no debate <LINE>\n",
      "<BOS> my arm's lumpy (i'm gastriferous) <LINE> didn't think it made me quite vociferous <LINE> i rubbed the bone in <LINE> gave me nodin and grin <LINE> is prone to be cruciferous, dentiferous <LINE>\n",
      "<BOS> any place where you live in bauling <LINE> has holes in the sea surrounding <LINE> if you go to the sea <LINE> you can find, can there be <LINE> the ocean of beauty's astounding <LINE>\n",
      "<BOS> the students are flying with flighty <LINE> they're so bashful; their manners are seedy <LINE> but they're rude as they do <LINE> is the fish that they chew <LINE> time to walk on just over forty <LINE>\n",
      "<BOS> anglacos: name given its name <LINE> to dracula's name was its game <LINE> and which garnered fame <LINE> so, but still is its name <LINE> as the name that's wild and not tame <LINE>\n"
     ]
    }
   ],
   "source": [
    "generate_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_length\": 100,\n",
    "}\n",
    "\n",
    "results = []\n",
    "for _ in range(100):\n",
    "    results.append(\n",
    "        generate_limericks(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            config,\n",
    "            [\"\"],\n",
    "            generate_params,\n",
    "            num_generation=1,\n",
    "            batch_size=1,\n",
    "            add_line_token=True)[0])\n",
    "\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve61g6NZSPyr"
   },
   "source": [
    "## Example of two-stage generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zXoU3BAB_F-"
   },
   "outputs": [],
   "source": [
    "standard_exp_dir = \"/content/drive/MyDrive/11-785-final/ckpt/bos-gpt2\"\n",
    "reverse_exp_dir = \"/content/drive/MyDrive/11-785-final/ckpt/reverse-bos-gpt2\"\n",
    "\n",
    "standard_config, standard_tokenizer, standard_model = \\\n",
    "    load_model(standard_exp_dir)\n",
    "reverse_config, reverse_tokenizer, reverse_model = \\\n",
    "    load_model(reverse_exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkjYE5yECA5c"
   },
   "outputs": [],
   "source": [
    "generate_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_length\": 100,\n",
    "}\n",
    "\n",
    "results = []\n",
    "for _ in range(100):\n",
    "    results.append(\n",
    "        generate_limericks_two_stage(\n",
    "            standard_model,\n",
    "            reverse_model,\n",
    "            standard_config,\n",
    "            reverse_config,\n",
    "            standard_tokenizer,\n",
    "            reverse_tokenizer,\n",
    "            [\"\"],\n",
    "            generate_params=generate_params,\n",
    "            num_generation_1=1,\n",
    "            num_generation_2=1,\n",
    "            batch_size=1)[0])\n",
    "\n",
    "for res in results:\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO/Y5yTT7CeYaAOFBkxIZz2",
   "collapsed_sections": [],
   "name": "GPT2_generation_github.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
