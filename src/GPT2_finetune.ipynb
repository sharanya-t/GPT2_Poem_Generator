{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtTnpB4Sg4Al"
   },
   "source": [
    "### Note\n",
    "You should be able to run the notebook on Colab after correctly mounting Google Drive. Notice that you may need to change some path names, and replace the Github access token to clone the private repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jt1R91Wqf0XP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.17.0 in /home/sthilaga/.local/lib/python3.9/site-packages (4.17.0)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sthilaga/.local/lib/python3.9/site-packages (from transformers==4.17.0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (2022.3.15)\n",
      "Requirement already satisfied: requests in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /home/sthilaga/.local/lib/python3.9/site-packages (from transformers==4.17.0) (0.1.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/sthilaga/.local/lib/python3.9/site-packages (from transformers==4.17.0) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers==4.17.0) (4.63.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sthilaga/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sthilaga/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.17.0) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers==4.17.0) (3.3)\n",
      "Requirement already satisfied: click in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hydra-core in /home/sthilaga/.local/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /home/sthilaga/.local/lib/python3.9/site-packages (from hydra-core) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/sthilaga/.local/lib/python3.9/site-packages (from hydra-core) (4.9.3)\n",
      "Requirement already satisfied: packaging in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from hydra-core) (21.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging->hydra-core) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "# Start by installing required libraries (mainly Transformers)\n",
    "!pip install transformers==4.17.0\n",
    "!pip install scikit-learn\n",
    "!pip install hydra-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0qTE4Ifh3dO"
   },
   "outputs": [],
   "source": [
    "# Only needed when running in colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsHm_8ENw_sq"
   },
   "outputs": [],
   "source": [
    "# you need to replace {your_own_token} with your own personal access token\n",
    "# Reference: https://stackoverflow.com/questions/48350226/methods-for-using-git-with-google-colab\n",
    "!git clone https://{your_own_token}@github.com/coderalo/11785-automatic-poetry-generation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KPIm6x10ipGI"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import string as string_utils\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import tqdm.notebook as tqdm\n",
    "import yaml\n",
    "\n",
    "from hydra import compose\n",
    "from hydra import initialize_config_dir\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Model\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AdamW, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aHS2lYxQ0Jqw"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sys.path.append(\"/content/11785-automatic-poetry-generation/\")\n",
    "\n",
    "# from src.dataset import merge_lines, reorder, reverse_line\n",
    "# from src.dataset import LimerickDataset\n",
    "# from src.utils import load_dataset, get_tokenizer\n",
    "\n",
    "from dataset import merge_lines, reorder, reverse_line\n",
    "from dataset import LimerickDataset\n",
    "from utils import load_dataset, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5wyIRoMA1KjE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244932/3351979110.py:11: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize_config_dir(config_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize_config_dir()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the path to your own shortcut\n",
    "\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "config_path = \"/scratch/sthilaga/GPT2_Poem_Generator/config/\"\n",
    "if not os.path.exists(config_path):\n",
    "    os.makedirs(config_path, exist_ok=True)\n",
    "    open(f\"{config_path}/__init__.py\", 'a').close()\n",
    "\n",
    "initialize_config_dir(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WHv7lGkTi8cp"
   },
   "outputs": [],
   "source": [
    "def load_dataset(config):\n",
    "    data = json.load(open(f\"/scratch/sthilaga/GPT2_Poem_Generator/data/preprocessing/limericks.json\"))\n",
    "    limericks = []\n",
    "\n",
    "    for _, limerick in data['limericks'].items():\n",
    "        lines = limerick['lines']\n",
    "        flag = True\n",
    "\n",
    "        # Remove the final punctuation of each line\n",
    "        # (we'll use a special separator instead)\n",
    "        for idx, line in enumerate(lines):\n",
    "            if len(line) == 0:\n",
    "                flag = False\n",
    "                break\n",
    "            if line[-1] in string_utils.punctuation:\n",
    "                lines[idx] = line[:-1]\n",
    "        \n",
    "        if flag:\n",
    "            limericks.append(lines)\n",
    "\n",
    "    print(f\"# of limericks before clean-up: {len(data['limericks'])}\")\n",
    "    print(f\"# of limericks after clean-up: {len(limericks)}\")\n",
    "\n",
    "    return limericks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zeowjCsk1xhv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  data_dir: /content/drive/MyDrive/11-785-final/data/\n",
      "  ckpt_dir: /content/drive/MyDrive/11-785-final/ckpt/\n",
      "  reverse: true\n",
      "  use_bos: true\n",
      "  order: null\n",
      "  punctuation: true\n",
      "training:\n",
      "  learning_rate: 5.0e-05\n",
      "  weight_decay: 0.0\n",
      "  scheduler_type: linear\n",
      "  num_warmup_steps: 0\n",
      "  epochs: 20\n",
      "  batch_size: 32\n",
      "  gradient_accumulation_steps: 1\n",
      "  full_train: false\n",
      "exp_name: reverse-gpt2\n",
      "debug: false\n",
      "device: cuda\n",
      "model_card: gpt2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finish configuration\n",
    "# change the path to your own shortcut\n",
    "config = compose(config_name=\"config\")\n",
    "config.exp_name = \"reverse-gpt2\"\n",
    "config.data.reverse = True\n",
    "config.data.use_bos = True\n",
    "# config.data.order = [0, 1, 4, 2, 3]\n",
    "config.data.punctuation = True\n",
    "config.training.epochs = 20\n",
    "\n",
    "assert config.exp_name is not None\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZbEWhlfJK0co"
   },
   "outputs": [],
   "source": [
    "# os.makedirs(config.data.ckpt_dir, exist_ok=True)\n",
    "exp_dir = f\"/scratch/sthilaga/GPT2_Poem_Generator/config/{config.exp_name}\"\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "log_file = f\"{exp_dir}/log.txt\"\n",
    "\n",
    "with open(f\"{exp_dir}/config.yaml\", 'w') as file:\n",
    "    file.write(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DiWAwnDvjAB-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of limericks before clean-up: 72432\n",
      "# of limericks after clean-up: 72431\n",
      "sep_token\n",
      "New sep_token: <LINE> (50257)\n",
      "pad_token\n",
      "New pad_token: <PAD> (50258)\n",
      "bos_token\n",
      "New bos_token: <BOS> (50259)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/scratch/sthilaga/GPT2_Poem_Generator/config/reverse-gpt2/tokenizer/tokenizer_config.json',\n",
       " '/scratch/sthilaga/GPT2_Poem_Generator/config/reverse-gpt2/tokenizer/special_tokens_map.json',\n",
       " '/scratch/sthilaga/GPT2_Poem_Generator/config/reverse-gpt2/tokenizer/vocab.json',\n",
       " '/scratch/sthilaga/GPT2_Poem_Generator/config/reverse-gpt2/tokenizer/merges.txt',\n",
       " '/scratch/sthilaga/GPT2_Poem_Generator/config/reverse-gpt2/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limericks = load_dataset(config)\n",
    "tokenizer = get_tokenizer(config)\n",
    "tokenizer.save_pretrained(f\"{exp_dir}/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V4runb51pXza"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_bos: True\n",
      "reverse: True\n",
      "line order: None\n",
      "Lines with separator: <BOS> dicrostonyx: genus of lemming <LINE> a beast of ignominy, stemming <LINE> from this myth: in a jiff <LINE> they all jump from a cliff <LINE> such mass suicide folks keep condemning <LINE>\n",
      "Tokens: [5.0259e+04 2.2290e+03 7.6000e+01 4.4300e+02 2.8600e+02 3.4306e+04\n",
      " 2.5000e+01 8.7000e+01 1.6470e+03 3.0100e+02 2.5000e+03 6.7000e+01\n",
      " 5.0257e+04 3.4807e+04 1.1000e+01 3.5410e+03 2.9600e+02 3.6270e+03\n",
      " 2.8600e+02 1.3824e+04 6.4000e+01 5.0257e+04 7.3300e+02 4.7400e+02\n",
      " 2.5700e+02 2.8700e+02 2.5000e+01 7.9180e+03 4.2800e+02 6.7380e+03\n",
      " 5.0257e+04 1.9516e+04 2.5700e+02 4.2200e+02 4.3910e+03 4.7700e+02\n",
      " 9.9300e+03 5.0257e+04 3.1568e+04 1.3940e+03 7.9740e+03 7.3410e+03\n",
      " 2.3470e+03 1.0508e+04 5.0257e+04]\n",
      "Decoding result: <BOS> mingm le of genus:xonysticrod <LINE>  stemming,inyom ign of beasta <LINE> iff j a in: myth thisfrom <LINE>  cliff a from jump allthey <LINE>  condemning keep folks suicide masssuch <LINE>\n"
     ]
    }
   ],
   "source": [
    "print(f\"use_bos: {config.data.use_bos}\")\n",
    "print(f\"reverse: {config.data.reverse}\")\n",
    "print(f\"line order: {config.data.order}\")\n",
    "\n",
    "sample = random.sample(limericks, 1)[0]\n",
    "string = merge_lines(sample, config.data.use_bos, config.data.order)\n",
    "print(f\"Lines with separator: {string}\")\n",
    "if config.data.reverse:\n",
    "    input_ids = reverse_line(\n",
    "        tokenizer(string)['input_ids'],\n",
    "        use_bos=config.data.use_bos,\n",
    "        tokenizer=tokenizer)\n",
    "else:\n",
    "    input_ids = list(tokenizer(string)['input_ids'])\n",
    "print(f\"Tokens: {input_ids}\")\n",
    "decoded_string = tokenizer.decode(input_ids)\n",
    "print(f\"Decoding result: {decoded_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_pImW1k9u6rp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training samples: 65187\n",
      "# of validation samples: 7244\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11785)\n",
    "random.seed(11785)\n",
    "\n",
    "if not config.training.full_train:\n",
    "    train_data, val_data = train_test_split(limericks, train_size=0.9)\n",
    "    if config.debug:\n",
    "        train_data = train_data[:config.training.batch_size * 8]\n",
    "        val_data = val_data[:config.training.batch_size * 2]\n",
    "    print(f\"# of training samples: {len(train_data)}\")\n",
    "    print(f\"# of validation samples: {len(val_data)}\")\n",
    "else:\n",
    "    train_data = limericks\n",
    "    if config.debug:\n",
    "        train_data = train_data[:config.training.batch_size * 8]\n",
    "    print(\"NOTE: USE ALL DATA FOR TRAINING\")\n",
    "    print(f\"# of training samples: {len(train_data)}\")\n",
    "\n",
    "train_dataset = LimerickDataset(train_data, config, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.gen_collate_fn())\n",
    "\n",
    "if not config.training.full_train:\n",
    "    val_dataset = LimerickDataset(val_data, config, tokenizer)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.training.batch_size,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        collate_fn=val_dataset.gen_collate_fn())\n",
    "else:\n",
    "    val_dataset, val_loader = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0xAPxUJ83W0I"
   },
   "outputs": [],
   "source": [
    "# initialize the model, also resize the embeddings for new tokens\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LM19-qJsvxwB"
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": config.training.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=config.training.learning_rate)\n",
    "\n",
    "T_epoch = np.ceil(\n",
    "    len(train_loader) //\n",
    "    config.training.gradient_accumulation_steps)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=config.training.scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.training.num_warmup_steps,\n",
    "    num_training_steps=config.training.epochs * T_epoch)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SpTOIuksLIi_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training from scratch\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(f\"{exp_dir}/epoch-*.ckpt\")\n",
    "if len(files) != 0:\n",
    "    files = sorted(files, key=lambda x: int(os.path.basename(x)[6:-5]))\n",
    "    states = torch.load(files[-1])\n",
    "    \n",
    "    model.load_state_dict(states['model_state_dict'])\n",
    "    optimizer.load_state_dict(states['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(states['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(states['scaler_state_dict'])\n",
    "    start_epoch = states['epoch'] + 1\n",
    "    best_perplexity = states['perplexity']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    if config.training.full_train:\n",
    "        best_perplexity = 0\n",
    "    else:\n",
    "        best_perplexity = 1e30\n",
    "\n",
    "if start_epoch == 0:\n",
    "    print(\"Start training from scratch\")\n",
    "else:\n",
    "    print(f\"Resume training from epoch {start_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "x7UvfFJoHuVa"
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, scaler):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bar = tqdm.tqdm(train_loader, leave=False)\n",
    "    loss_total = 0.\n",
    "\n",
    "    for step, batch in enumerate(bar):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss_total += loss.item()\n",
    "        loss = loss / config.training.gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "  \n",
    "        if (\n",
    "                step % config.training.gradient_accumulation_steps == 0 or\n",
    "                step == len(train_loader) - 1\n",
    "        ):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        bar.set_postfix({\"Loss\": f\"{loss_total / (step + 1):.4f}\"})\n",
    "\n",
    "    return loss_total / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P6EwklrvJMRX"
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "def validation(model, val_loader):\n",
    "    model.eval()\n",
    "\n",
    "    bar = tqdm.tqdm(val_loader, leave=False)\n",
    "    losses = []\n",
    "\n",
    "    for step, batch in enumerate(bar):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        batch_size = batch['input_ids'].shape[0]\n",
    "        loss = outputs.loss.item()\n",
    "        losses.extend([loss for _ in range(batch_size)])\n",
    "\n",
    "        try:\n",
    "            perplexity = math.exp(np.mean(losses))\n",
    "        except OverflowError:\n",
    "            perplexity = float('inf')\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PpaU1ITp4VbO"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.8259 Perplexity 66.4498\n",
      "Save model at epoch 1\n",
      "Save best model at epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 4.0675 Perplexity 49.0870\n",
      "Save model at epoch 2\n",
      "Save best model at epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 3.8121 Perplexity 40.5012\n",
      "Save model at epoch 3\n",
      "Save best model at epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 3.6097 Perplexity 35.1068\n",
      "Save model at epoch 4\n",
      "Save best model at epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 3.4671 Perplexity 32.0107\n",
      "Save model at epoch 5\n",
      "Save best model at epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 3.3441 Perplexity 30.0299\n",
      "Save model at epoch 6\n",
      "Save best model at epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 3.2548 Perplexity 28.5316\n",
      "Save model at epoch 7\n",
      "Save best model at epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 3.1879 Perplexity 27.5812\n",
      "Save model at epoch 8\n",
      "Save best model at epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 3.1180 Perplexity 26.8321\n",
      "Save model at epoch 9\n",
      "Save best model at epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 3.0680 Perplexity 26.4759\n",
      "Save model at epoch 10\n",
      "Save best model at epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 3.0189 Perplexity 26.0686\n",
      "Save model at epoch 11\n",
      "Save best model at epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 2.9807 Perplexity 25.8615\n",
      "Save model at epoch 12\n",
      "Save best model at epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 2.9451 Perplexity 25.6578\n",
      "Save model at epoch 13\n",
      "Save best model at epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 2.9133 Perplexity 25.4981\n",
      "Save model at epoch 14\n",
      "Save best model at epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 2.8838 Perplexity 25.3878\n",
      "Save model at epoch 15\n",
      "Save best model at epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 2.8613 Perplexity 25.3956\n",
      "Save model at epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 2.8423 Perplexity 25.3711\n",
      "Save model at epoch 17\n",
      "Save best model at epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 2.8250 Perplexity 25.3874\n",
      "Save model at epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 2.8149 Perplexity 25.3963\n",
      "Save model at epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 2.8094 Perplexity 25.4287\n",
      "Save model at epoch 20\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "epoch_bar = tqdm.trange(start_epoch, config.training.epochs, leave=False)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    loss = train_epoch(model, train_loader, optimizer, scheduler, scaler)\n",
    "    flag = False\n",
    "\n",
    "    if config.training.full_train:\n",
    "        perplexity = 0\n",
    "        log = f\"Epoch {epoch+1} Loss: {loss:.4f}\"\n",
    "    else:\n",
    "        perplexity = validation(model, val_loader)\n",
    "        log = f\"Epoch {epoch+1} Loss: {loss:.4f} Perplexity {perplexity:.4f}\"\n",
    "     \n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            flag = True\n",
    "\n",
    "    epoch_bar.write(log)\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(f\"{log}\\n\")\n",
    "\n",
    "    epoch_bar.write(f\"Save model at epoch {epoch+1}\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': \n",
    "            scheduler.state_dict()\n",
    "            if scheduler is not None else None,\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'perplexity': perplexity,\n",
    "        'best_perplexity': best_perplexity\n",
    "    }, f\"{exp_dir}/epoch-{epoch+1}.ckpt\")\n",
    "    if epoch != 0:\n",
    "        prev_ckpt = f\"{exp_dir}/epoch-{epoch}.ckpt\"\n",
    "        if os.path.exists(prev_ckpt):\n",
    "            os.remove(f\"{exp_dir}/epoch-{epoch}.ckpt\")\n",
    "\n",
    "    if flag or config.training.full_train:\n",
    "        print(f\"Save best model at epoch {epoch+1}\")\n",
    "        best_perplexity = perplexity\n",
    "        shutil.copyfile(\n",
    "            f\"{exp_dir}/epoch-{epoch+1}.ckpt\",\n",
    "            f\"{exp_dir}/best-model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKiBhEAHvqkDKpUeIu52MY",
   "collapsed_sections": [],
   "name": "GPT2_finetune_github.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
